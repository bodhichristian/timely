{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c29a75b0947f616",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "Leveraging the dataset found in github_issues_processed.csv we are going to train an ML model capable of making predictions for issue category as well as issue labels.\n",
    "\n",
    "This is a step towards a predictive experience for users that are submitting a GitHub issue. after the user has update the title and body of the issue report, issue category will be predicted and recommended with a confidence score. the user will also be presented with up to 3 label suggestions for the issue, each presented with a confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5b96e28403bb3",
   "metadata": {},
   "source": [
    "### Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95605874e5a73995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T23:53:43.736552Z",
     "start_time": "2025-08-17T23:53:42.666543Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"github_issues_processed.csv\")\n",
    "# df.head(1).to_json('github_issues_processed.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8614e9118879b",
   "metadata": {},
   "source": [
    "### Define targets, drop source and helper features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb007982180ebb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T23:49:19.278714Z",
     "start_time": "2025-08-17T23:49:19.191716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Category target\n",
    "cat_targets = [\n",
    "    \"is_bug_cat\",\"is_feature_cat\",\"is_doc_cat\",\n",
    "    \"is_help_cat\",\"is_priority_cat\",\"is_status_cat\"\n",
    "]\n",
    "df[\"category\"] = df[cat_targets].idxmax(axis=1)\n",
    "\n",
    "# Label targets\n",
    "label_targets = [\n",
    "    col for col in df.columns if col.startswith(\"has_\")\n",
    "]\n",
    "\n",
    "# Define features, exlude targets and helper features\n",
    "exclude = cat_targets + label_targets + [\"n_labels\", \"category\"]\n",
    "X = df.drop(columns=exclude)\n",
    "y_cat = df[\"category\"]\n",
    "y_labels = df[label_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327325b",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_cat_train, y_cat_test, y_labels_train, y_labels_test = train_test_split(\n",
    "    X, y_cat, y_labels, test_size=0.2, random_state=42, stratify=y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db862e",
   "metadata": {},
   "source": [
    "### Define base configurations for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_config = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced_subsample'\n",
    "}\n",
    "\n",
    "gb_config = {\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_config = {\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 2,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'multi:softprob'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bcecb",
   "metadata": {},
   "source": [
    "### Create and evaluate RandomForest pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e98947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with SMOTE\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     is_bug_cat       0.97      0.96      0.96      2961\n",
      "     is_doc_cat       0.36      0.44      0.40        79\n",
      " is_feature_cat       0.47      0.47      0.47       119\n",
      "    is_help_cat       0.00      0.00      0.00         3\n",
      "is_priority_cat       0.00      0.00      0.00         1\n",
      "  is_status_cat       0.69      0.71      0.70        28\n",
      "\n",
      "       accuracy                           0.93      3191\n",
      "      macro avg       0.41      0.43      0.42      3191\n",
      "   weighted avg       0.93      0.93      0.93      3191\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "          feature  importance\n",
      "94       tfidf_69    0.024419\n",
      "24   repo_encoded    0.018853\n",
      "43       tfidf_18    0.018852\n",
      "529      bert_254    0.016776\n",
      "59       tfidf_34    0.016533\n",
      "518      bert_243    0.015620\n",
      "566      bert_291    0.014023\n",
      "206     tfidf_181    0.013225\n",
      "278        bert_3    0.012186\n",
      "631      bert_356    0.012151\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=2)),\n",
    "    ('rf', RandomForestClassifier(**rf_config))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "rf_pipeline.fit(X_train, y_cat_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_proba_rf = rf_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Print detailed evaluation\n",
    "print(\"Random Forest with SMOTE\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_cat_test, y_pred_rf, zero_division=0))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_pipeline.named_steps['rf'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45341fcf",
   "metadata": {},
   "source": [
    "### Create and evaluate Gradient Boosting pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c2df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting with SMOTE\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     is_bug_cat       0.96      0.98      0.97      2961\n",
      "     is_doc_cat       0.39      0.35      0.37        79\n",
      " is_feature_cat       0.59      0.40      0.48       119\n",
      "    is_help_cat       0.00      0.00      0.00         3\n",
      "is_priority_cat       0.00      0.00      0.00         1\n",
      "  is_status_cat       0.90      1.00      0.95        28\n",
      "\n",
      "       accuracy                           0.94      3191\n",
      "      macro avg       0.48      0.46      0.46      3191\n",
      "   weighted avg       0.93      0.94      0.94      3191\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                  feature  importance\n",
      "59               tfidf_34    0.112645\n",
      "24           repo_encoded    0.110441\n",
      "94               tfidf_69    0.066607\n",
      "45               tfidf_20    0.065836\n",
      "635              bert_360    0.038837\n",
      "3    n_days_to_resolution    0.037265\n",
      "478              bert_203    0.031430\n",
      "112              tfidf_87    0.029732\n",
      "197             tfidf_172    0.029240\n",
      "486              bert_211    0.024485\n"
     ]
    }
   ],
   "source": [
    "gb_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=2)),\n",
    "    ('gb', GradientBoostingClassifier(**gb_config))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "gb_pipeline.fit(X_train, y_cat_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "y_proba_gb = gb_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Print detailed evaluation\n",
    "print(\"Gradient Boosting with SMOTE\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_cat_test, y_pred_gb, zero_division=0))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': gb_pipeline.named_steps['gb'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_gb.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34e767",
   "metadata": {},
   "source": [
    "### Create and evaluate XGBoost pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f76e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with SMOTE\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     is_bug_cat       0.96      0.99      0.97      2961\n",
      "     is_doc_cat       0.52      0.35      0.42        79\n",
      " is_feature_cat       0.64      0.35      0.45       119\n",
      "    is_help_cat       0.00      0.00      0.00         3\n",
      "is_priority_cat       0.00      0.00      0.00         1\n",
      "  is_status_cat       0.90      1.00      0.95        28\n",
      "\n",
      "       accuracy                           0.94      3191\n",
      "      macro avg       0.50      0.45      0.47      3191\n",
      "   weighted avg       0.93      0.94      0.94      3191\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "       feature  importance\n",
      "59    tfidf_34    0.048010\n",
      "211  tfidf_186    0.032082\n",
      "206  tfidf_181    0.030725\n",
      "45    tfidf_20    0.030340\n",
      "446   bert_171    0.027569\n",
      "112   tfidf_87    0.025247\n",
      "518   bert_243    0.019160\n",
      "82    tfidf_57    0.018136\n",
      "499   bert_224    0.018078\n",
      "250  tfidf_225    0.017043\n"
     ]
    }
   ],
   "source": [
    "# Get the number of classes for XGBoost\n",
    "n_classes = len(np.unique(y_cat_train))\n",
    "xgb_config['num_class'] = n_classes\n",
    "\n",
    "# Create and fit the label encoder\n",
    "le = LabelEncoder()\n",
    "y_cat_train_encoded = le.fit_transform(y_cat_train)\n",
    "y_cat_test_encoded = le.transform(y_cat_test)\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=2)),\n",
    "    ('xgb', xgb.XGBClassifier(**xgb_config))\n",
    "])\n",
    "\n",
    "# Fit the model with encoded labels\n",
    "xgb_pipeline.fit(X_train, y_cat_train_encoded)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred_xgb_encoded = xgb_pipeline.predict(X_test)\n",
    "y_proba_xgb = xgb_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
    "\n",
    "# Print detailed evaluation\n",
    "print(\"XGBoost with SMOTE\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_cat_test, y_pred_xgb, zero_division=0))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': xgb_pipeline.named_steps['xgb'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_xgb.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e8d32a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Comparison:\n",
      "                RandomForest  GradientBoosting  XGBoost\n",
      "accuracy              0.9270            0.9398   0.9448\n",
      "precision             0.9293            0.9329   0.9349\n",
      "recall                0.9270            0.9398   0.9448\n",
      "f1                    0.9280            0.9356   0.9378\n",
      "avg_confidence        0.5443            0.9510   0.9543\n",
      "\n",
      "Best performing model based on F1 score: XGBoost\n",
      "F1 Score: 0.9378\n"
     ]
    }
   ],
   "source": [
    "# Compare model performances\n",
    "def get_model_metrics(y_true, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Get per-class probabilities\n",
    "    class_probs = np.max(y_proba, axis=1)\n",
    "    avg_confidence = np.mean(class_probs)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'avg_confidence': avg_confidence\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each model\n",
    "rf_metrics = get_model_metrics(y_cat_test, y_pred_rf, y_proba_rf)\n",
    "gb_metrics = get_model_metrics(y_cat_test, y_pred_gb, y_proba_gb)\n",
    "xgb_metrics = get_model_metrics(y_cat_test, y_pred_xgb, y_proba_xgb)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'RandomForest': rf_metrics,\n",
    "    'GradientBoosting': gb_metrics,\n",
    "    'XGBoost': xgb_metrics\n",
    "}).round(4)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Identify best model based on F1 score\n",
    "best_model = metrics_df.loc['f1'].idxmax()\n",
    "print(f\"\\nBest performing model based on F1 score: {best_model}\")\n",
    "print(f\"F1 Score: {metrics_df.loc['f1', best_model]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "824398d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Issue Triage Example:\n",
      "{\n",
      "  \"primary_category\": {\n",
      "    \"category\": \"is_bug_cat\",\n",
      "    \"confidence\": 0.469302645865518,\n",
      "    \"action_needed\": true\n",
      "  },\n",
      "  \"secondary_suggestions\": [\n",
      "    {\n",
      "      \"category\": \"is_feature_cat\",\n",
      "      \"confidence\": 0.2594492256654643,\n",
      "      \"action_needed\": false\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"is_doc_cat\",\n",
      "      \"confidence\": 0.24299169460957643,\n",
      "      \"action_needed\": false\n",
      "    }\n",
      "  ],\n",
      "  \"triage_recommendations\": [\n",
      "    {\n",
      "      \"type\": \"low_confidence\",\n",
      "      \"message\": \"Low confidence prediction - Manual review recommended\",\n",
      "      \"priority\": \"medium\"\n",
      "    }\n",
      "  ],\n",
      "  \"repo_context\": {\n",
      "    \"repository\": \"auth-service\",\n",
      "    \"typical_response_time\": \"2-3 days\",\n",
      "    \"similar_issues_count\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Smart Issue Triage Feature\n",
    "def get_issue_recommendations(text, repo, model=rf_pipeline, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Provides smart recommendations for GitHub issues including:\n",
    "    - Primary category prediction with confidence\n",
    "    - Secondary category suggestions\n",
    "    - Confidence-based recommendations\n",
    "    - Similar issue detection (if confidence is low)\n",
    "    \n",
    "    Args:\n",
    "        text: The issue title + description\n",
    "        repo: Repository name\n",
    "        model: Trained model pipeline\n",
    "        threshold: Confidence threshold for recommendations\n",
    "    \n",
    "    Returns:\n",
    "        dict: Recommendations and insights\n",
    "    \"\"\"\n",
    "    # Get model predictions and probabilities\n",
    "    proba = model.predict_proba(X_test)[0]  # Using first test example for demo\n",
    "    classes = model.classes_\n",
    "    \n",
    "    # Sort predictions by confidence\n",
    "    pred_confidence = list(zip(classes, proba))\n",
    "    pred_confidence.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Prepare recommendations\n",
    "    recommendations = {\n",
    "        'primary_category': {\n",
    "            'category': pred_confidence[0][0],\n",
    "            'confidence': float(pred_confidence[0][1]),\n",
    "            'action_needed': True if pred_confidence[0][0] in ['is_bug_cat', 'is_priority_cat'] else False\n",
    "        },\n",
    "        'secondary_suggestions': [\n",
    "            {\n",
    "                'category': cat,\n",
    "                'confidence': float(conf),\n",
    "                'action_needed': True if cat in ['is_bug_cat', 'is_priority_cat'] else False\n",
    "            }\n",
    "            for cat, conf in pred_confidence[1:3] if conf > threshold\n",
    "        ],\n",
    "        'triage_recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Add triage recommendations based on predictions\n",
    "    if recommendations['primary_category']['category'] == 'is_bug_cat':\n",
    "        if recommendations['primary_category']['confidence'] > 0.9:\n",
    "            recommendations['triage_recommendations'].append({\n",
    "                'type': 'high_confidence_bug',\n",
    "                'message': 'High confidence bug report - Consider immediate review',\n",
    "                'priority': 'high'\n",
    "            })\n",
    "    elif recommendations['primary_category']['category'] == 'is_feature_cat':\n",
    "        recommendations['triage_recommendations'].append({\n",
    "            'type': 'feature_request',\n",
    "            'message': 'Feature request - Add to product backlog review',\n",
    "            'priority': 'medium'\n",
    "        })\n",
    "    elif recommendations['primary_category']['category'] == 'is_doc_cat':\n",
    "        recommendations['triage_recommendations'].append({\n",
    "            'type': 'documentation',\n",
    "            'message': 'Documentation issue - Tag for docs team review',\n",
    "            'priority': 'medium'\n",
    "        })\n",
    "    \n",
    "    # Add confidence-based recommendations\n",
    "    if recommendations['primary_category']['confidence'] < 0.5:\n",
    "        recommendations['triage_recommendations'].append({\n",
    "            'type': 'low_confidence',\n",
    "            'message': 'Low confidence prediction - Manual review recommended',\n",
    "            'priority': 'medium'\n",
    "        })\n",
    "    \n",
    "    # Add repository-specific insights\n",
    "    if 'repo_encoded' in X_test.columns:\n",
    "        recommendations['repo_context'] = {\n",
    "            'repository': repo,\n",
    "            'typical_response_time': '2-3 days',  # This could be calculated from historical data\n",
    "            'similar_issues_count': 5  # This could be calculated using embedding similarity\n",
    "        }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "example_text = \"Error in login flow: users cannot reset password\"\n",
    "example_repo = \"auth-service\"\n",
    "\n",
    "print(\"Smart Issue Triage Example:\")\n",
    "print(json.dumps(get_issue_recommendations(example_text, example_repo), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5279828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Issue Triage Results\n",
      "===================\n",
      "\n",
      "Input Text: Error in login flow: users cannot reset password\n",
      "Repository: auth-service\n",
      "\n",
      "Primary Category:\n",
      "- is_bug_cat\n",
      "- Confidence: 46.93%\n",
      "- Action Needed: True\n",
      "\n",
      "Secondary Suggestions:\n",
      "- is_feature_cat (Confidence: 25.94%)\n",
      "- is_doc_cat (Confidence: 24.30%)\n",
      "\n",
      "Triage Recommendations:\n",
      "- [MEDIUM] Low confidence prediction - Manual review recommended\n",
      "\n",
      "Repository Context:\n",
      "- Typical Response Time: 2-3 days\n",
      "- Similar Open Issues: 5\n",
      "\n",
      "==================================================\n",
      "Issue Triage Results\n",
      "===================\n",
      "\n",
      "Input Text: Add dark mode support to dashboard\n",
      "Repository: frontend-ui\n",
      "\n",
      "Primary Category:\n",
      "- is_bug_cat\n",
      "- Confidence: 46.93%\n",
      "- Action Needed: True\n",
      "\n",
      "Secondary Suggestions:\n",
      "- is_feature_cat (Confidence: 25.94%)\n",
      "- is_doc_cat (Confidence: 24.30%)\n",
      "\n",
      "Triage Recommendations:\n",
      "- [MEDIUM] Low confidence prediction - Manual review recommended\n",
      "\n",
      "Repository Context:\n",
      "- Typical Response Time: 2-3 days\n",
      "- Similar Open Issues: 5\n",
      "\n",
      "==================================================\n",
      "Issue Triage Results\n",
      "===================\n",
      "\n",
      "Input Text: Update API documentation for new endpoints\n",
      "Repository: api-service\n",
      "\n",
      "Primary Category:\n",
      "- is_bug_cat\n",
      "- Confidence: 46.93%\n",
      "- Action Needed: True\n",
      "\n",
      "Secondary Suggestions:\n",
      "- is_feature_cat (Confidence: 25.94%)\n",
      "- is_doc_cat (Confidence: 24.30%)\n",
      "\n",
      "Triage Recommendations:\n",
      "- [MEDIUM] Low confidence prediction - Manual review recommended\n",
      "\n",
      "Repository Context:\n",
      "- Typical Response Time: 2-3 days\n",
      "- Similar Open Issues: 5\n",
      "\n",
      "==================================================\n",
      "Issue Triage Results\n",
      "===================\n",
      "\n",
      "Input Text: High CPU usage in production environment\n",
      "Repository: backend-service\n",
      "\n",
      "Primary Category:\n",
      "- is_bug_cat\n",
      "- Confidence: 46.93%\n",
      "- Action Needed: True\n",
      "\n",
      "Secondary Suggestions:\n",
      "- is_feature_cat (Confidence: 25.94%)\n",
      "- is_doc_cat (Confidence: 24.30%)\n",
      "\n",
      "Triage Recommendations:\n",
      "- [MEDIUM] Low confidence prediction - Manual review recommended\n",
      "\n",
      "Repository Context:\n",
      "- Typical Response Time: 2-3 days\n",
      "- Similar Open Issues: 5\n"
     ]
    }
   ],
   "source": [
    "# Preprocess new issues for prediction\n",
    "def preprocess_issue(text, repo):\n",
    "    \"\"\"\n",
    "    Preprocesses a new issue for prediction using the same pipeline as training data\n",
    "    \"\"\"\n",
    "    # Create a single sample DataFrame\n",
    "    sample = pd.DataFrame({\n",
    "        'text': [text],\n",
    "        'repo': [repo]\n",
    "    })\n",
    "    \n",
    "    # Apply the same preprocessing as training data\n",
    "    # Note: This would need access to the original preprocessing pipeline\n",
    "    # For now, we'll use the first test sample as an example\n",
    "    return X_test.iloc[[0]]\n",
    "\n",
    "def smart_issue_triage(text, repo):\n",
    "    \"\"\"\n",
    "    End-to-end issue triage function that:\n",
    "    1. Preprocesses the issue\n",
    "    2. Gets model predictions\n",
    "    3. Provides actionable recommendations\n",
    "    \"\"\"\n",
    "    # Preprocess the issue\n",
    "    processed_issue = preprocess_issue(text, repo)\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = get_issue_recommendations(processed_issue, repo)\n",
    "    \n",
    "    # Format output for display\n",
    "    print(\"Issue Triage Results\")\n",
    "    print(\"===================\")\n",
    "    print(f\"\\nInput Text: {text}\")\n",
    "    print(f\"Repository: {repo}\")\n",
    "    print(\"\\nPrimary Category:\")\n",
    "    print(f\"- {recommendations['primary_category']['category']}\")\n",
    "    print(f\"- Confidence: {recommendations['primary_category']['confidence']:.2%}\")\n",
    "    print(f\"- Action Needed: {recommendations['primary_category']['action_needed']}\")\n",
    "    \n",
    "    if recommendations['secondary_suggestions']:\n",
    "        print(\"\\nSecondary Suggestions:\")\n",
    "        for suggestion in recommendations['secondary_suggestions']:\n",
    "            print(f\"- {suggestion['category']} (Confidence: {suggestion['confidence']:.2%})\")\n",
    "    \n",
    "    print(\"\\nTriage Recommendations:\")\n",
    "    for rec in recommendations['triage_recommendations']:\n",
    "        print(f\"- [{rec['priority'].upper()}] {rec['message']}\")\n",
    "    \n",
    "    if 'repo_context' in recommendations:\n",
    "        print(\"\\nRepository Context:\")\n",
    "        print(f\"- Typical Response Time: {recommendations['repo_context']['typical_response_time']}\")\n",
    "        print(f\"- Similar Open Issues: {recommendations['repo_context']['similar_issues_count']}\")\n",
    "\n",
    "# Example usage\n",
    "example_issues = [\n",
    "    (\"Error in login flow: users cannot reset password\", \"auth-service\"),\n",
    "    (\"Add dark mode support to dashboard\", \"frontend-ui\"),\n",
    "    (\"Update API documentation for new endpoints\", \"api-service\"),\n",
    "    (\"High CPU usage in production environment\", \"backend-service\")\n",
    "]\n",
    "\n",
    "for text, repo in example_issues:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    smart_issue_triage(text, repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47453c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
